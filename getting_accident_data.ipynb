{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial attempt at getting data from website using scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got links: set()\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/questions/44187490/downloading-files-from-a-website-using-python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def file_links_filter(tag):\n",
    "    \"\"\"\n",
    "    Tags filter. Return True for links that ends with 'pdf', 'htm' or 'txt'\n",
    "    \"\"\"\n",
    "    if isinstance(tag, str):\n",
    "        return tag.endswith('pdf') or tag.endswith('zip') or tag.endswith('txt')\n",
    "\n",
    "\n",
    "def get_links(tags_list):\n",
    "    return [WEB_ROOT + tag.attrs['href'] for tag in tags_list]\n",
    "\n",
    "\n",
    "def download_file(file_link, folder):\n",
    "    file = requests.get(file_link).content\n",
    "    name = file_link.split('/')[-1]\n",
    "    save_path = folder + name\n",
    "\n",
    "    print(\"Saving file:\", save_path)\n",
    "    with open(save_path, 'wb') as fp:\n",
    "        fp.write(file)\n",
    "\n",
    "\n",
    "WEB_ROOT = 'https://unfallatlas.statistikportal.de'\n",
    "SAVE_FOLDER = './data/'  # directory in which files will be downloaded\n",
    "\n",
    "r = requests.get(\"https://unfallatlas.statistikportal.de/_opendata2022.html\")\n",
    "\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "years = soup.select(\"p#archive-links > a\")  # css selector for all <a> inside <p id='archive'> tag\n",
    "years_links = get_links(years)\n",
    "\n",
    "links_to_download = []\n",
    "for year_link in years_links:\n",
    "    page = requests.get(year_link)\n",
    "    beautiful_page = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    links = beautiful_page.find_all(\"a\", href=file_links_filter)\n",
    "    links = get_links(links)\n",
    "\n",
    "    links_to_download.extend(links)\n",
    "\n",
    "# make set to exclude duplicate links\n",
    "links_to_download = set(links_to_download)\n",
    "\n",
    "print(\"Got links:\", links_to_download)\n",
    "\n",
    "for link in set(links_to_download):\n",
    "    download_file(link, SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abe290aac7a6bfdd6ff5b1727087f8416d8784ccfe70428b0457475e252cf8cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
